# DABERT: Replacing BERT's attention layers with mini linear DNNs 
 Re-writing State of the Union addresses with Harry Potter

# Project by : Liam Patty, Alexis Ballo, Alexander Nichols

# Credits & Foundational Code Sources :

[Base Model](https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial) 

[File Reading](https://github.com/rtealwitter/dl-demos/blob/b537a5dd94953ea656a2140227af78f67c042540/demo08-word2vec.ipynb)

[Linear/Sequential Dimensions](https://theaisummer.com/transformer/)

[Activation Layers](https://pytorch.org/docs/master/special.html)

[Original Paper](https://arxiv.org/pdf/1810.04805.pdf)
